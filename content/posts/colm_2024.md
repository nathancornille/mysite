+++
title = "Learning to Plan for Language Modeling from Unlabeled Data"
date = 2024-08-02
tags = []
description = ""
custom_css= ["/css/custom.css"]
+++

<!---
GOAL
 - ? Make website which currently only has blog post for first paper up-to-date?
    - but that would imply a blog post for second paper also
 - Have a more accessible presentation of the paper ideas that will engage people seeing my twitter/linkedin post about the paper more
STRUCTURE
 - Problem setting
    - LMs are very useful, and a self-supervised pretraining objective is key, because it is scalable
    - Standard LMs still simply predict the next word left-to-right, which seems counter to how humans would write more complex texts: we think ahead at an abstract level about what we want to say, and then write based on this plan
    - our goal was to train a planner to make abstract future writing plans, with properties:
        - being self-supervised, so it is scalable
        - in latent space rather than text space, as it is richer
- Approach
  - main figure
  - 3 parts
    - generating abstract writing plans in a self-supervised way
        - we generate sentence embeddings, and cluster those to have a finite set of what we will call 'writing actions'
    - training a planner to predict these writing actions
        - we train a planner to predict the next writing action given the preceding text
          - we found a planner that first contextualizes within-sentence to produce an embedding per sentence, and then across sentences, to work well
    - training an LM to use predicted writing actions to improve perplexity
        - we try two variants: one trained on oracle actions (where we might expect a train/test mismatch), and one trained on predicted actions (where the actions will be less informative about the future text)
- Results
  - Evaluation metrics
    - our main metric of interest is perplexity: the typical metric for LMs
    - We also evaluate generation: both surface-level metrics (ROUGE-2 measuring n-gram overlap with true text, and MAUVE comparing the true and predict distribution for open-ended text generation), and abstract-level metrics: converting true and generate text into a sequence of their corresponding high-level writing actions, and comparing these sequences (with Edit distance and Latent Perplexity). Details about the metrics can be found in the paper.
  - our main goal was to see whether the LM could benefit from the planner predictions
    - because we start with models pre-trained on a different dataset than our fine-tuning data (which is English Wikipedia), we need to control for the effect of adaptation. We do so by comparing to a model that is fine-tuned in a similar way, but while only receiving fixed (and so uninformative) writing actions
    - we find that
      - indeed we can consistently outperform this Fixed/Fixed baseline in both perplexity and generation metrics
      - training the LM on predicted actions benefits perplexity, which makes sense given the lack of train/test mismatch
      - training on oracle actions leads the LM to "follow its own plan" more often. Following your plan means that the cluster of the sentence you generated corresponds to the writing actions used to condition the generation on. This is around 40% for oracle actions compared to 20% for predicted actions.
      - despite having higher perplexity, training the LM on oracle actions is sometimes better for the generation metrics. We hypothesize this is due to its superior plan-following capacity. Finetuning the LM in such a way that it prevents a train/test mismatch _and_ more closely follows the plan is a promising direction for future work.
- What now?
  - While our results are encouraging, our planner might not take the world by storm just yet. For one, it is currently only planning one step ahead. If we can learn to plan multiple steps ahead, we could benefit from search algorithms during generation. For another, the planner-predicted actions might benefit from being tailored to a specific LM. Lots of room for future work!

-->

In this blog post, I'll give a bite-sized summary of our paper [Learning to Plan for Language Modeling from Unlabeled Data](https://arxiv.org/pdf/2404.00614), which was accepted at COLM 2024  ðŸ¥³.

## Problem Setting

Language Models (LMs) have revolutionized various NLP tasks, largely due to their self-supervised pretraining objectives that scale effectively with data. However, traditional LMs predict the next word in a sequence purely left-to-right, which contrasts with how humans approach writing complex texts. We typically plan ahead, coming up with ideas at an abstract level before putting them into words.

Our research aimed to bridge this gap by developing a planner that can generate abstract writing plans. These plans have two key features:
- **Scalability through self-supervision**: Like modern LMs, the planner can learn from large amounts of unlabeled data.
- **Expressiveness by operating in a latent space**: Instead of planning in text space, which can be restrictive, our planner operates in a more expressive latent space.


## Approach
Our approach is divided into three main components, as illustrated below:

<img src="/colm_main_fig.png" alt="Main Figure" width="50%">

We start by generating sentence embeddings and then clustering these embeddings to define a set of 'writing actions'. These actions serve as the building blocks for abstract writing plans.

Next, we train a planner to predict the next writing action based on the preceding text. Our experiments showed that a planner that first processes within-sentence context to create sentence embeddings and then operates across sentences performed the best.

Finally, we integrate the predicted writing actions into the language model. We explored two training variants:
- **Oracle actions**: Using ground-truth actions, knowing this could introduce a train/test mismatch.
- **Predicted actions**: Using the plannerâ€™s predicted actions, which are less precise but avoid the mismatch.


## Results

### Evaluation Metrics

We assessed our approach using several metrics:
- **Perplexity**: The standard metric for evaluating LMs, reflecting how well the model predicts the next word.
- **Surface-level generation metrics**: ROUGE-2 (measuring n-gram overlap with reference text) and MAUVE (comparing distribution of generated versus reference text).
- **Abstract-level generation metrics**: We converted both reference and generated text into sequences of high-level writing actions and compared these using Edit distance and Latent Perplexity.

### Key Findings

Our primary goal was to determine whether the language model could benefit from the plannerâ€™s predictions. Hereâ€™s what we discovered:

[//]: # (- We consistently outperform the Fixed/Fixed baseline in both perplexity and generation metrics.)

[//]: # (- Training the LM on predicted actions benefits perplexity, which makes sense given the lack of train/test mismatch.)

[//]: # (- Training on oracle actions leads the LM to "follow its own plan" more often. Following your plan means that the cluster of the sentence you generated corresponds to the writing actions used to condition the generation on. This is around 40% for oracle actions compared to 20% for predicted actions.)

[//]: # (- Despite having higher perplexity, training the LM on oracle actions is sometimes better for the generation metrics. We hypothesize this is due to its superior plan-following capacity. Finetuning the LM in such a way that it prevents a train/test mismatch and more closely follows the plan is a promising direction for future work.)
- **Improved Performance**: Our models consistently outperformed the baseline (which used fixed, uninformative actions) in both perplexity and generation metrics.
- **Perplexity Benefits**: Training the LM on predicted actions led to better perplexity scores, likely due to the absence of train/test mismatch.
- **Enhanced Plan-Following**: Training on oracle actions increased the likelihood that the LM would "follow its own plan," with around 40% alignment compared to 20% when using predicted actions.
- **Generation Quality**: Despite worse perplexity, models trained on oracle actions sometimes performed better on generation metrics than model trained on predicted actions, likely due to stronger plan-following. This suggests that fine-tuning LMs to minimize train/test mismatch while maintaining plan fidelity is a promising direction for future research.

## What Now?

While our findings are encouraging, our planner might not take the world by storm just yet. For one, it currently only planw one step ahead. Extending this to multi-step planning could open the door to leveraging search algorithms during text generation. Additionally, tailoring planner-predicted actions to specific LMs could further enhance performance. Thereâ€™s plenty of exciting work ahead!

For a deeper dive, check out our full paper at [https://arxiv.org/pdf/2404.00614](https://arxiv.org/pdf/2404.00614).
hugo
